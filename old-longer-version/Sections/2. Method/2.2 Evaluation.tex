% 2. Method
% 2.2 Evaluation
\subsection{Evaluation}

The methodology in evaluating the quality of a model’s segmentation in this work follows Chris Fournier's \emph{Evaluating Text Segmentation using Boundary Edit Distance}~\cite{fournier-2013-B}. This work uses edited versions of the Boundary Similarity proposed along with associated information recall metrics: `Boundary Precision’ and ‘Boundary Recall’. 

The boundary edit distance algorithm pairs matches of segment boundaries between a reference segmentation and a hypothesised segmentation regardless of distance between matches. Exact matches score 1 and boundaries without matches score 0, whilst matches within some distance $n$ score partial points as a function of distance. For this work, the function always decreases score linearly as the boundary gets farther away from the true boundary. Boundary Similarity (B) is the mean score for all these matches, while Boundary Precision/Recall (BP/BR) measure the proportion of the hypothesis/reference boundaries which are matched, respectively. 

A model is punished for both missing a boundary and for placing a boundary where there is none in a symmetric and intuitive manner. For further justification of the usage of the boundary edit distance algorithm as opposed to more traditional metrics such as WD and Pk~\cite{HearstW2002}, see \emph{Evaluating Text Segmentation using Boundary Edit Distance}~\cite{fournier-2013-B}, or see our own investigations at \href{https://github.com/PierreRL/segmenter-evaluation-metrics}{segmenter evaluation metrics}.