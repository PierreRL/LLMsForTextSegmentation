@article{TextTiling,
  author     = {Hearst, Marti A.},
  title      = {TextTiling: segmenting text into multi-paragraph subtopic passages},
  year       = {1997},
  issue_date = {March 1997},
  publisher  = {MIT Press},
  address    = {Cambridge, MA, USA},
  volume     = {23},
  number     = {1},
  issn       = {0891-2017},
  abstract   = {TextTiling is a technique for subdividing texts into multi-paragraph units that represent passages, or subtopics. The discourse cues for identifying major subtopic shifts are patterns of lexical co-occurrence and distribution. The algorithm is fully implemented and is shown to produce segmentation that corresponds well to human judgments of the subtopic boundaries of 12 texts. Multi-paragraph subtopic segmentation should be useful for many text analysis tasks, including information retrieval and summarization.},
  journal    = {Comput. Linguist.},
  month      = {mar},
  pages      = {33–64},
  numpages   = {32}
}

@article{EffectOfLongContextWindows,
  title     = {Lost in the Middle: How Language Models Use Long Contexts},
  author    = {Liu, Nelson F.  and
               Lin, Kevin  and
               Hewitt, John  and
               Paranjape, Ashwin  and
               Bevilacqua, Michele  and
               Petroni, Fabio  and
               Liang, Percy},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {12},
  year      = {2024},
  address   = {Cambridge, MA},
  publisher = {MIT Press},
  url       = {https://aclanthology.org/2024.tacl-1.9},
  doi       = {10.1162/tacl_a_00638},
  pages     = {157--173},
  abstract  = {While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.}
}

@article{HearstW2002,
  title     = {A Critique and Improvement of an Evaluation Metric for Text Segmentation},
  author    = {Pevzner, Lev  and
               Hearst, Marti A.},
  journal   = {Computational Linguistics},
  volume    = {28},
  number    = {1},
  year      = {2002},
  address   = {Cambridge, MA},
  publisher = {MIT Press},
  url       = {https://aclanthology.org/J02-1002},
  doi       = {10.1162/089120102317341756},
  pages     = {19--36}
}

@inproceedings{fournier-2013-B,
  title     = {Evaluating Text Segmentation using Boundary Edit Distance},
  author    = {Fournier, Chris},
  editor    = {Schuetze, Hinrich  and
               Fung, Pascale  and
               Poesio, Massimo},
  booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = aug,
  year      = {2013},
  address   = {Sofia, Bulgaria},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/P13-1167},
  pages     = {1702--1712}
}

@misc{ExtendingContextWindows,
  title         = {Extending Context Window of Large Language Models via Positional Interpolation},
  author        = {Shouyuan Chen and Sherman Wong and Liangjian Chen and Yuandong Tian},
  year          = {2023},
  eprint        = {2306.15595},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2306.15595}
}

@inproceedings{ContextAffectsFactual,
  author = {Petroni, F and Lewis, PSH and Piktus, A and Rocktäschel, Tim and Wu, Yuxiang and Miller, AH and Riedel, Sebastian},
  year   = {2020},
  month  = {06},
  title  = {How Context Affects Language Models' Factual Predictions}
}

@inproceedings{FewShotLearners,
  author    = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  title     = {Language models are few-shot learners},
  year      = {2020},
  isbn      = {9781713829546},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  abstract  = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
  booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
  articleno = {159},
  numpages  = {25},
  location  = {Vancouver, BC, Canada},
  series    = {NIPS '20}
}

@phdthesis{XingThesis,
  series     = {Electronic Theses and Dissertations (ETDs) 2008+},
  title      = {Versatile neural approaches to more accurate and robust topic segmentation},
  url        = {https://open.library.ubc.ca/collections/ubctheses/24/items/1.0440128},
  doi        = {http://dx.doi.org/10.14288/1.0440128},
  school     = {University of British Columbia},
  author     = {Xing, Linzi},
  year       = {2024},
  collection = {Electronic Theses and Dissertations (ETDs) 2008+}
}

@misc{MasimilianoSegmenter,
  author = {Massimiliano Costacurta},
  title  = {Text Tiling Done Right: Building Solid Foundations For Your Personal LLM},
  year   = {2023},
  doi    = {https://towardsdatascience.com/text-tiling-done-right-building-solid-foundations-for-your-personal-llm-e70947779ac1}
}

@inproceedings{SentenceBERT,
  title     = {Sentence-{BERT}: Sentence Embeddings using {S}iamese {BERT}-Networks},
  author    = {Reimers, Nils  and
               Gurevych, Iryna},
  editor    = {Inui, Kentaro  and
               Jiang, Jing  and
               Ng, Vincent  and
               Wan, Xiaojun},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  month     = nov,
  year      = {2019},
  address   = {Hong Kong, China},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/D19-1410},
  doi       = {10.18653/v1/D19-1410},
  pages     = {3982--3992},
  abstract  = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.}
}

@article{FlanT5,
  author    = {Hyung Won Chung and
               Le Hou and
               Shayne Longpre and
               Barret Zoph and
               Yi Tay and
               William Fedus and
               Yunxuan Li and
               Xuezhi Wang and
               Mostafa Dehghani and
               Siddhartha Brahma and
               Albert Webson and
               Shixiang Shane Gu and
               Zhuyun Dai and
               Mirac Suzgun and
               Xinyun Chen and
               Aakanksha Chowdhery and
               Alex Castro{-}Ros and
               Marie Pellat and
               Kevin Robinson and
               Dasha Valter and
               Sharan Narang and
               Gaurav Mishra and
               Adams Yu and
               Vincent Y. Zhao and
               Yanping Huang and
               Andrew M. Dai and
               Hongkun Yu and
               Slav Petrov and
               Ed H. Chi and
               Jeff Dean and
               Jacob Devlin and
               Adam Roberts and
               Denny Zhou and
               Quoc V. Le and
               Jason Wei},
  title     = {Scaling Instruction-Finetuned Language Models},
  journal   = {J. Mach. Learn. Res.},
  volume    = {25},
  pages     = {70:1--70:53},
  year      = {2024},
  url       = {https://jmlr.org/papers/v25/23-0870.html},
  timestamp = {Mon, 16 Sep 2024 01:00:00 +0200},
  biburl    = {https://dblp.org/rec/journals/jmlr/ChungHLZTFL00BW24.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{LORA,
  title     = {Lo{RA}: Low-Rank Adaptation of Large Language Models},
  author    = {Edward J Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
  booktitle = {International Conference on Learning Representations},
  year      = {2022},
  url       = {https://openreview.net/forum?id=nZeVKeeFYf9}
}

@inproceedings{lexical1,
  title     = {Discourse Segmentation of Multi-Party Conversation},
  author    = {Galley, Michel  and
               McKeown, Kathleen R.  and
               Fosler-Lussier, Eric  and
               Jing, Hongyan},
  booktitle = {Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics},
  month     = jul,
  year      = {2003},
  address   = {Sapporo, Japan},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/P03-1071},
  doi       = {10.3115/1075096.1075167},
  pages     = {562--569}
}

@inproceedings{lexical2,
  title     = {Hierarchical Text Segmentation from Multi-Scale Lexical Cohesion},
  author    = {Eisenstein, Jacob},
  editor    = {Ostendorf, Mari  and
               Collins, Michael  and
               Narayanan, Shri  and
               Oard, Douglas W.  and
               Vanderwende, Lucy},
  booktitle = {Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics},
  month     = jun,
  year      = {2009},
  address   = {Boulder, Colorado},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N09-1040},
  pages     = {353--361}
}

@inproceedings{BiLSTM,
  title     = {Learning Distributed Word Representations For Bidirectional {LSTM} Recurrent Neural Network},
  author    = {Wang, Peilu  and
               Qian, Yao  and
               Soong, Frank K.  and
               He, Lei  and
               Zhao, Hai},
  editor    = {Knight, Kevin  and
               Nenkova, Ani  and
               Rambow, Owen},
  booktitle = {Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = jun,
  year      = {2016},
  address   = {San Diego, California},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N16-1064},
  doi       = {10.18653/v1/N16-1064},
  pages     = {527--533}
}


@inproceedings{HierarchicalBiLSTM,
  title     = {Text Segmentation as a Supervised Learning Task},
  author    = {Koshorek, Omri  and
               Cohen, Adir  and
               Mor, Noam  and
               Rotman, Michael  and
               Berant, Jonathan},
  editor    = {Walker, Marilyn  and
               Ji, Heng  and
               Stent, Amanda},
  booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
  month     = jun,
  year      = {2018},
  address   = {New Orleans, Louisiana},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N18-2075},
  doi       = {10.18653/v1/N18-2075},
  pages     = {469--473},
  abstract  = {Text segmentation, the task of dividing a document into contiguous segments based on its semantic structure, is a longstanding challenge in language understanding. Previous work on text segmentation focused on unsupervised methods such as clustering or graph search, due to the paucity in labeled data. In this work, we formulate text segmentation as a supervised learning problem, and present a large new dataset for text segmentation that is automatically extracted and labeled from Wikipedia. Moreover, we develop a segmentation model based on this dataset and show that it generalizes well to unseen natural text.}
}

@inproceedings{CrossAttentionHierarchical,
  title     = {Text Segmentation by Cross Segment Attention},
  author    = {Lukasik, Michal  and
               Dadachev, Boris  and
               Papineni, Kishore  and
               Sim{\~o}es, Gon{\c{c}}alo},
  editor    = {Webber, Bonnie  and
               Cohn, Trevor  and
               He, Yulan  and
               Liu, Yang},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  month     = nov,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.emnlp-main.380},
  doi       = {10.18653/v1/2020.emnlp-main.380},
  pages     = {4707--4716},
  abstract  = {Document and discourse segmentation are two fundamental NLP tasks pertaining to breaking up text into constituents, which are commonly used to help downstream tasks such as information retrieval or text summarization. In this work, we propose three transformer-based architectures and provide comprehensive comparisons with previously proposed approaches on three standard datasets. We establish a new state-of-the-art, reducing in particular the error rates by a large margin in all cases. We further analyze model sizes and find that we can build models with many fewer parameters while keeping good performance, thus facilitating real-world applications.}
}

@inproceedings{CNNFeaturesLSTMAttention,
  author    = {Badjatiya, Pinkesh
               and Kurisinkel, Litton J.
               and Gupta, Manish
               and Varma, Vasudeva},
  editor    = {Pasi, Gabriella
               and Piwowarski, Benjamin
               and Azzopardi, Leif
               and Hanbury, Allan},
  title     = {Attention-Based Neural Text Segmentation},
  booktitle = {Advances in Information Retrieval},
  year      = {2018},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {180--193},
  isbn      = {978-3-319-76941-7}
}

@article{TwoLevelTransformerSoftmax,
  author  = {Glava s, Goran and Somasundaran, Swapna},
  year    = {2020},
  month   = {04},
  pages   = {7797-7804},
  title   = {Two-Level Transformer and Auxiliary Coherence Modeling for Improved Text Segmentation},
  volume  = {34},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  doi     = {10.1609/aaai.v34i05.6284}
}


@inproceedings{TwoLevelTransformerPretrained,
  title     = {Transformer over Pre-trained Transformer for Neural Text Segmentation with Enhanced Topic Coherence},
  author    = {Lo, Kelvin  and
               Jin, Yuan  and
               Tan, Weicong  and
               Liu, Ming  and
               Du, Lan  and
               Buntine, Wray},
  editor    = {Moens, Marie-Francine  and
               Huang, Xuanjing  and
               Specia, Lucia  and
               Yih, Scott Wen-tau},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2021},
  month     = nov,
  year      = {2021},
  address   = {Punta Cana, Dominican Republic},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.findings-emnlp.283},
  doi       = {10.18653/v1/2021.findings-emnlp.283},
  pages     = {3334--3340}
}

@inproceedings{DialoGPT,
  title     = {Language Model as an Annotator: Exploring {D}ialo{GPT} for Dialogue Summarization},
  author    = {Feng, Xiachong  and
               Feng, Xiaocheng  and
               Qin, Libo  and
               Qin, Bing  and
               Liu, Ting},
  editor    = {Zong, Chengqing  and
               Xia, Fei  and
               Li, Wenjie  and
               Navigli, Roberto},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  month     = aug,
  year      = {2021},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.acl-long.117},
  doi       = {10.18653/v1/2021.acl-long.117},
  pages     = {1479--1491},
  abstract  = {Current dialogue summarization systems usually encode the text with a number of general semantic features (e.g., keywords and topics) to gain more powerful dialogue modeling capabilities. However, these features are obtained via open-domain toolkits that are dialog-agnostic or heavily relied on human annotations. In this paper, we show how DialoGPT, a pre-trained model for conversational response generation, can be developed as an unsupervised dialogue annotator, which takes advantage of dialogue background knowledge encoded in DialoGPT. We apply DialoGPT to label three types of features on two dialogue summarization datasets, SAMSum and AMI, and employ pre-trained and non pre-trained models as our summarizers. Experimental results show that our proposed method can obtain remarkable improvements on both datasets and achieves new state-of-the-art performance on the SAMSum dataset.}
}