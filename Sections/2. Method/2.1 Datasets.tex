% 2. Method
% 2.1 Datasets
\subsection{Datasets}

% There are 4 datasets used in the experiments in this work: a small human-annotated dataset, a scrape of English wikipedia, a `concatenated' wikipedia scrape and a synthetic GPT3.5 generated dataset.

\emph{Human}: We use a small dataset of 10 manually segmented documents. It is comprised of miscellaneous news/wikpedia/scientific articles. This was intended to to provide high quality examples for qualitative analysis.

\emph{Wiki}: A wikipedia scrape\footnote{\url{https://www.kaggle.com/datasets/ltcmdrdata/plain-text-wikipedia-202011/data}} was automatically segmented based on headings and filtered to remove articles with too few segments (<4), too short segments (<20 words) or too many artefacts (> 20\% non-alphabetic characters), leaving ~1000 articles. We evaluate without headings present.

\emph{Conc-Wiki}: We randomly sampled segments from \emph{Wiki} and concatenated them to form new incoherent articles, with segments drawn from completely different domains, leading to ~500 articles.

\emph{Synthetic}: Segmentations were generated by \emph{GPT-3.5}~(\ref{GPT3.5}) on proprietary source data consisting of technical/news reports. This dataset was used for both fine-tuning \emph{FlanT5} (\ref{FlanT5}) and evaluation.