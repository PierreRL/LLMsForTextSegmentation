\subsection{Related Work}

\citep{XingThesis} provides a recent, broad overview of topic segmentation. Their work discusses the history of segmentation, dialogue/hierarchical/multi-modal segmentation, as well as exploration of large language models for topic segmentation.

Methods prior to the era of neural networks often relied on lexical similarity metrics. An influential framework introduced by~\citep{TextTiling} in their paper \emph{TextTiling} involves computing lexical similarity scores between adjacent sentences before boundaries are placed where similarity scores are lowest. A variety of such lexical similarity metrics were proposed~\citep{lexical1,lexical2}, but were proven to be too superficial compared to semanic similarity that came after it. Such a framework is still in use today but with semantic similarity metrics generated by embeddings from language models like SentenceBERT~\citep{SentenceBERT}.

Neural networks have seen much use for topic segmentation. The task can be conceived as a sequence labelling task and as such, BiLSTMs have been employed for this purpose~\citep{BiLSTM} as well as attention-based methods~\cite{CrossAttentionHierarchical}. Many methods leverage a hierarchical framework in which one model extracts sequential features from the text, and another model constructs sentence boundaries based on these features. These models have also been LSTMs~\cite{HierarchicalBiLSTM,CNNFeaturesLSTMAttention}, or more recently transformers~\citep{TwoLevelTransformerSoftmax,TwoLevelTransformerPretrained}. Of these, the most recent a large pre-trained transformer for feature extraction, followed by a transformers classifier. However, these methods may not take full advantage of rich representations contained in the largest models as there is a bottleneck introduced by the hierarchical structure. We propose that an end-to-end  method may be more effective.

LLMs have been the state of the art for a variety of NLP tasks including translation, summarisation and information extraction for a number of years now. However, there has been little research into the usage of LLMs for topic segmentation. A loss-based approach was proposed by~\citep{DialoGPT} for annotation of dialogues and then extended by~\citep{XingThesis}. This approach computes the mean NLL (Negative Log Likelihood) token-wise loss of a pre-trained model when predicting the tokens in a sentence. Boundaries are then placed where this loss is above some threshold, indicating that the sentence was hard to predict. While this approach is almost directly able to leverage the vast knowledge contained in LLMs and a tune-able threshold is appealing, it is also an arbitrary threshold, and there remains a strong inductive bias that the loss will be higher at segment boundaries, even in nuanced cases.

The only work we found to explore the use of directly prompting an LLM for topic segmentation was~\citep{XingThesis}, whose paper on the subject is currently under review. The find that prompting ChatGPT is the best dialogue segmentation model unless the input exceeds ChatGPT's limit. They propose two prompting methods: one which asks the LLM to return the original model with special characters delimiting the segment boundary, and one in which the LLM is asked to return pair-wise semantic coherence scores in the range of 0 to 1 for adjacent sentences. The first method falls short of our requirements as there is no guarantee that the model will return the original document unedited, and is wasteful of tokens. The second method is methodologically flawed as the returned scores have no guarantee of directly corresponding to semantic coherence \emph{for topic segmentation}, are very hard to interpret and there will still be a bottleneck in turning scores into segment boundaries. We propose a new prompting method which we believe is more effective and more interpretable, less wasteful of tokens, and not limited by the maximum token limit of the model.

In this work, we compare our new prompting work to our own existing method which uses SentenceBERT embeddings and cosine similarity to decide on segment boundaries. We do not compare to other prompting methods nor methods based on hierarchites of transformers due to resource constraints. However, our focus is on whether our new prompting strategy is a feasible replacement for methods which used semantic similarity. Further work should more thoroughly compare our new prompting method to other prompting schema, to loss-based approaches and to other hierarchical methods.