% 3.2 Results

\begin{table*}[ht]
    \centering
    \begin{tabular}{@{}lcccc@{\hspace{2em}}ccccc@{\hspace{0.2em}}c@{}}
    \toprule
    & \multicolumn{4}{c}{Boundary Similarity ($n=2$)} & \multicolumn{6}{c}{Boundary Precision and Recall} \\ 
    \midrule
    & Human & Wiki & Conc-Wiki & Synthetic & \multicolumn{2}{c}{Human} & \multicolumn{2}{c}{Wiki} & \multicolumn{2}{c}{Conc-Wiki} \\ 
    &  &  &  &  & BP & BR & BP & BR & BP & BR \\
    \midrule
    \it GPT3.5  & \bf 0.38 & \bf 0.25 & 0.29 & \bf 0.35 & \bf 0.51 & \bf 0.60 & 0.36 & \bf 0.55 & 0.42 & \bf 0.63 \\ 
    \it FlanT5  & 0.25 & 0.24 & 0.41 & 0.33 & 0.38 & 0.46 & \bf 0.43 & 0.37 & 0.65 & \bf 0.63 \\ 
    \midrule
    \it BERTGraph   & 0.20 & 0.15 & 0.45 & 0.21 & 0.47 & 0.25 & 0.39 & 0.21 & 0.79 & 0.54 \\ 
    \it BERT    & 0.18 & 0.09 & \bf 0.46 & 0.18 & 0.33 & 0.39 & 0.23 & 0.37 & \bf 0.91 & 0.50 \\ 
    \it RandomF0.1 & 0.09 & 0.11 & 0.10 & 0.10 & 0.16 & 0.21 & 0.34 & 0.16 & 0.20 & 0.16 \\ 
    \it Split5  & 0.13 & 0.19 & 0.19 & 0.23 & 0.17 & 0.48 & 0.33 & 0.39 & 0.25 & 0.52 \\ 
    \bottomrule
    \end{tabular}
    \caption{Results for Boundary Similarity, Boundary Precision (BP) and Recall (BR) with $n=2$ for each model and dataset. Best results for each metric are highlighted in \bf bold.}~\label{tab:combined_results}
\end{table*}


\subsection{Quantitative Results}

Models were tested on the human-annotated dataset, the wikipedia dataset, the concatenated wikipedia dataset and a test-partition of the synthetic dataset. The fine-tuned model was not trained on this partition. We do not base our conclusions on results from the synthetic dataset as they would be biased in favor of the generative models which generated the segmentations. Indeed, while performance of the LLMs on the synthetic dataset is far from perfect (a boundary similarity of less than 0.5 with $n=2$), it is disproportionately better than other models compared to other datasets.

We evaluate primarily using the previously discussed boundary similarity metric with $n=2$. Results can be seen in Table~\ref{tab:combined_results}. Evaluation was also run with boundary similarity $n=5$ but the metric becomes noisier with higher values of $n$. Indeed, the naive baselines report better performance than some other models with high enough $n$. 

Due to resource constraints, we could not test \emph{GPT3.5} on the full wikipedia or full concatenated wikipedia datasets. Instead, we took the largest subset that fit within resource constraints. We evaluated all other models on the full datasets to verify that similar results are obtained. Full results on all evalutation metrics can be found online at the following links for both the smaller \href{https://docs.google.com/spreadsheets/d/15CJhNuioTS9L13tXsGfaG54igEF1HB6Os_mnR0k4P-I/edit?usp=sharing}{smaller} and \href{https://docs.google.com/spreadsheets/d/1UorM9wHrSOVURJ5djldZ6y3kCjNMNcQSFEBOALo-HgI/edit?usp=sharing}{larger} datasets.

We see that \emph{GPT3.5} outperforms all other models on the human-annotated dataset, the wikipedia dataset, and the synthetic dataset, with the other LLM \emph{FlanT5} coming in second in the same datasets. The close performance between the models on the synthetic dataset implies that \emph{FlanT5} is a good approximation of \emph{GPT3.5} for this task, on boundaries generate by \emph{GPT3.5}. However, the biggest performance gap is on the human-annotated dataset. Although this dataset is small, it represents a meaningful distribution shift and, unsurprisingly, the fine-tuned model is unable to generalize as well as the base model.

Interestingly, we see that both BERT-based models are superior in the purportedly easier task posed by the concatenated wikipedia dataset. We theorise that these models are better at finding clear boundaries between different domains, but struggle with more nuanced segment boundaries found in a news article, for example. Whereas, the LLMs are better at finding more nuanced segments that a human might have produced but are not significantly better at finding more clear-cut boundaries.



We also looked at precision and recall to better characterize the behavior of each model. Results can be found in the second half of Table~\ref{tab:combined_results}. \emph{GPT3.5} has the highest precision and recall in the human-annotated dataset and the highest recall on both the wiki and concatenated wiki datasets. This property of having high recall is true in general for the LLMs, even in the concatenated wiki dataset where the BERT models have higher overall boundary similarity. By contrast, precision scores are closer in general and much better for the BERT models on the concatenated wiki dataset. This suggests that in general the BERT models are more hesitant in placing boundaries but when they do they are more likely to be correct. It should be noted that this behaviour in both the LLMs and BERT models is subject to the prompt engineering and segment processing/validation procedures used.

    

\subsection{Qualitative Evaluation}

Manual testing and inspection of the segmentations produced by the models was also conducted, both during prompt engineering and after the models were trained.

Through manual inspection of segmentations with the human-annotated dataset, we found that \emph{GPT3.5} generally found the boundaries which seemed reasonable from a human perspective, especially for simple documents like short news articles. The fine-tuned \emph{FlanT5} model imitated this behavior, but was less consistent. BERT segmenters would find reasonable segments, but after the manual gluing and splitting procedure, would often lead to off-by-1 errors.

For documents with far more complex or nuanced text such as a podcast transcript, or with messy data like tables or artefacts from pdf to text conversions, \emph{GPT3.5} would sometimes return indices with a regular pattern. For example, the LLM might return '$[1,15,22, \ldots, 76, 79, 82, 85, 88, \ldots, 184, 187, \ldots]$'. Often, the pattern would continue far beyond the number of sentences in the input indicating that the model became stuck in a regular pattern. Perhaps better prompt engineering, a more rigorous data-processing procedure, the use of newer models or the use of better generation parameters would help. However, our current approach was resource constrained but still required the ability to pass noisy documents to the model. A more thorough investigation of the logits computed by the model is required to understand how and when this occurs, and how to mitigate it.