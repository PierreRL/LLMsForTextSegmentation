% 3.1 Results
\subsection{Models}

We use two naive baselines as points of reference. First, the \emph{Split5} segmenter splits every 5 sentences. Second, a \emph{RandomF0.1} segmenter which splits at 10\% of boundaries, placed uniformly at random.

\emph{BERT}: Our current method generates a sequence of similarities using embeddings from SBERT~\cite{SentenceBERT}. Similarities are a weighted average of the cosine similarities with the previous $n$ sentences. Boundaries are placed at troughs in the sequence before long segments are split and short segments are grouped.

\emph{BERTGraph}:~\cite{MasimilianoSegmenter} also uses SBERT-generated similarity scores, but instead uses graph clustering to find the best segments and post-processing ensures that the clusters are valid segments. Code was copied from the repository linked in the article where futher details can be found.

\emph{GPT3.5}\label{GPT3.5}: OpenAI's \texttt{gpt-3.5-turbo-16k}\footnote{Queries were made in August 2023.} was queried using the method defined in Section~\ref{LLM-Based Text Segmentation}. We choose to use deterministic outputs from the model by setting topk $=1$. 

\emph{FlanT5}\label{FlanT5}: We fine-tune Google's Flan-T5 large~\citep{FlanT5} (780M) using LORA~\citep{LORA} on a combination of \emph{Wiki}, \emph{Conc-Wiki} and \emph{Synthetic}. Fine-tuning took place until loss plateauted, after approximately 48 GPU hours. We used the same topk $=1$ as \emph{GPT3.5}. Because the model has been fine-tuned we use a much shorter prompt and no few-shot examples.