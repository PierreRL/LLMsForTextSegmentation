% 3.1 Results
\subsection{Models}

\subsubsection{Baselines}

We use two naive baselines as a point of reference. First, a segmenter which splits every n sentences. We decided to split every 5 sentences which we call the \emph{Split5Segmenter}. We also define a \emph{RandomF0.1Segmenter} which splits at 10\% of boundaries, placed randomly, with each potential boundary equally likely.

\subsubsection{BERT Segmenter}

Our existing method generates a sequence of sentence similarities using sentence embeddings generated by \cite{SentenceBERT}. Similarities are calculated as a weighted sum of the cosine similarity to the previous $n$ sentences. Ideal boundaries are then generated as troughs in the sequence of similarities, before further processing to ensure there are no segments that are too long or too short, either in sentence length or in token length. We name this the \emph{BERTSegmenter}. Further details are omitted for proprietary reasons.

\subsubsection{BERT-Graph Segmenter}

\cite{MasimilianoSegmenter} describes 'text tiling' (topic segmentation) using BERT-generated similarity scores followed by graph clustering to find the best segments. This follows a similar methodology as the previous Exact details of this method can be found in the linked article. This model is called \emph{BERTGraphSegmenter} in our experiments. Code for this model was copied from the repository linked in the article.

\subsubsection{GPT-3.5}

OpenAI's \texttt{gpt-3.5-turbo-16k} was queried using the prompting and segment validation strategy defined in Section 2. We call this model \emph{GPT3.5}. We used the largest model available to us, which is the 16k token context window. Given the nature of the task, we chose to use deterministic outputs from the model, using topk $=1$, rather than sampling. This makes sense as we are looking for the best possible segmentations, rather than a variety of possible segmentations, and helps with reproducibility of results.

\subsubsection{Flan-T5-Finetuned}\label{FlanT5}

Our intended application of segmentation requires us to use our own models, therefore we fine-tune Google's Flan-T5 large~\citep{FlanT5} on a combination of wiki, concatenated wiki and synthetic segmentations generated by GPT-3.5. We call this model \emph{FlanT5}. We used the same deterministic output and a similar prompting strategy as with \emph{GPT3.5}. Because the model has been fine-tuned, we use a much shorter instruction section of a prompt with no examples. Additionally, given the model's shorter context window, we have many more overlapping prompt per document. 