% 3.1 Results
\subsection{Models}

\subsubsection{Baselines}

We use two naive baselines as a point of reference. First, a segmenter which splits every n sentences. We decided to split every 5 sentences which we call the \emph{Split5Segmenter}. We also define a \emph{RandomF0.1Segmenter} which splits at 10\% of boundaries, placed randomly, with each potential boundary equally likely.

\subsubsection{BERT Segmenter}

Our existing method generates a sequence of sentence similarities using sentence embeddings generated by \cite{SentenceBERT}. Similarities are calculated as a weighted sum of the cosine similarity to the previous $n$ sentences. Ideal boundaries are then generated as troughs in the sequence of similarities, before further processing to ensure there are no segments that are too long or too short, either in sentence length or in token length. We name this the \emph{BERTSegmenter}. Further details are omitted for proprietary reasons.

\subsubsection{BERT-Graph Segmenter}

\cite{MasimilianoSegmenter} describes 'text tiling' (topic segmentation) using BERT-generated similarity scores followed by graph clustering to find the best segments. This follows a similar methodology as the previous Exact details of this method can be found in the linked article. This model is called \emph{BERTGraphSegmenter} in our experiments. Code for this model was copied from the repository linked in the article.

\subsubsection{GPT-3.5}

OpenAI's \texttt{gpt-3.5-turbo-16k} was queried using the prompting and segment validation strategy defined in Section 2. We call this model \emph{GPT3.5}. We used the largest model available to us, which is the 16k token context window.

\subsubsection{Flan-T5-Finetuned}

Took Flan-T5 large [XX] and fine-tuned it a combination of wiki, concatenated wiki and synthetic data.