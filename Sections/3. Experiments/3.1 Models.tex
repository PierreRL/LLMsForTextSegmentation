% 3.1 Results
\subsection{Models}

\subsubsection{Baselines}

We use two naive baselines as a point of reference. First, a segmenter which splits every $n$ sentences. Based on preliminary testing, we decided to split every 5 sentences which we call the \emph{Split5} segmenter. We also define a \emph{RandomF0.1} segmenter which splits at 10\% of boundaries, placed randomly, with each potential boundary equally likely.

\subsubsection{BERT Segmenter}

Our existing method generates a sequence of sentence similarities using sentence embeddings generated by~\cite{SentenceBERT}. Similarities are calculated as a weighted sum of the cosine similarity to the previous $n$ sentences. Ideal boundaries are then generated as troughs in the sequence of similarities, before post-processing of segments ensures that no segment is too long or too short, either in sentence length or in token length. We name this the \emph{BERT} segmenter. Further details are omitted for proprietary reasons.

\subsubsection{BERT-Graph Segmenter}

~\cite{MasimilianoSegmenter} describes `text tiling' (topic segmentation) also using BERT-generated similarity scores followed by graph clustering to find the best segments and post-processing to ensure that the clusters return valid segments. Further details of this method can be found in the linked article. This model is called \emph{BERTGraph} segmenter in our experiments. Code for this model was copied from the repository linked in the article.

\subsubsection{GPT-3.5}

OpenAI's \texttt{gpt-3.5-turbo-16k} was queried using the prompting and segment validation strategy defined in Section 2. We call this model \emph{GPT3.5}. We used the largest model available to us, which is the 16k token context window. Note that this was done during the summer of 2023; the cheapest tier of model has increased in capabilities since then. Given the nature of the task, we chose to use deterministic outputs from the model, using topk $=1$, rather than sampling. We are looking for the best possible segmentations, rather than a variety of possible segmentations, and this also helps with reproducibility of segmentations.

\subsubsection{Flan-T5-Finetuned}\label{FlanT5}

Our intended application of segmentation requires us to use our own models, therefore we fine-tune Google's Flan-T5 large~\citep{FlanT5} on a combination of wiki, concatenated wiki and synthetic segmentations generated by GPT-3.5 using LORA~\citep{LORA}. We call this model \emph{FlanT5}. We used the same deterministic output generation parameters as with \emph{GPT3.5}. Because the model has been fine-tuned, we use a much shorter prompt with a brief instruction and no few-shot examples. Additionally, given the model's shorter context window, we have many more overlapping prompts per document.